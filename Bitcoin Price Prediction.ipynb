{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ed3e6a-1062-4c25-ab66-9dd51c3ab1d2",
   "metadata": {},
   "source": [
    "<h1><center>Bitcoin Price Forecasting using LSTM Neural Networks</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456420bf-bfe9-4911-830c-7ccdd5b8f4be",
   "metadata": {},
   "source": [
    "<h3><center>CE418 - Neuro-Fuzzy Computing</center></h3> \n",
    "<h3><center>Aslanidou Aikaterina-Sofia </center></h3>  \n",
    "<h3><center>University of Thessaly</center></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed12b1-5615-44b0-809e-297493236095",
   "metadata": {},
   "source": [
    "<h3> About This Notebook </h3>        \r",
    "<b>This notebook presents the implementation of our project, including concise explanations and inline comments where necessary. A more detailed discussion, analysis, and evaluation of the results can be found in the accompanying report.</b>.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657498ab-de45-4d97-8cee-a14e01b70a1f",
   "metadata": {},
   "source": [
    "<h4>Importing Necessary Libraries</h4>    \n",
    "In this section, we import the essential Python libraries required for data preprocessing, model training, evaluation, and visualization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a3342461-0e57-4280-aa0c-81f246b1dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984513f1-3fc1-42ac-9050-edac7959d0c2",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error (MAPE) Function\n",
    "The mean_absolute_percentage_error function calculates the Mean Absolute Percentage Error (MAPE) between the true values (y_true) and the predicted values (y_pred). This metric expresses the error as a percentage, making it useful for evaluating prediction accuracy in regression problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3b141a98-393b-450b-a8da-644315645646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94bfda-e69a-4a84-8515-d34b8f8f029f",
   "metadata": {},
   "source": [
    "### DataLoader class\n",
    "The DataLoader class is responsible for loading, processing, and formatting Bitcoin price data for use in an LSTM-based neural network. It reads data from a CSV file and ensures the date column is in datetime format, sorts the data chronologically, selects only the date and close price columns, handles missing values, and resamples the data to an hourly frequency using forward fill. Additionally, it applies MinMax scaling to normalize the close prices. The class provides methods to filter and scale data within a specific date range, create sequences of sequence_length for time series prediction, split the data into training, validation, and test sets, and verify generated sequences with raw close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f599d4bd-6f74-4468-a9ff-ad66b4efc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, filepath):\n",
    "        print(\"=== Initializing DataLoader ===\")\n",
    "        self.data = pd.read_csv(filepath)\n",
    "\n",
    "        # Make sure 'date' is recognized as datetime\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'], errors='coerce')\n",
    "\n",
    "        # Sort by date\n",
    "        self.data.sort_values('date', inplace=True)\n",
    "        print(f\"Initial CSV shape: {self.data.shape}\")\n",
    "\n",
    "        # Keep only the columns we need\n",
    "        self.data = self.data[['date', 'close']]\n",
    "\n",
    "        # Check for any NA in 'close'\n",
    "        na_count = self.data['close'].isna().sum()\n",
    "        print(f\"NaNs in 'close' before resampling: {na_count}\")\n",
    "\n",
    "        # Set date as index and resample to hourly frequency\n",
    "        self.data.set_index('date', inplace=True)\n",
    "        self.data = self.data.resample('h').ffill()  \n",
    "        print(f\"Shape after resampling: {self.data.shape}\")\n",
    "\n",
    "        # Reset index back to columns\n",
    "        self.data.reset_index(inplace=True)\n",
    "        print(\"First few rows after resampling + forward-fill:\")\n",
    "        print(self.data.head(5))\n",
    "\n",
    "        # Initialize MinMaxScaler\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    def format_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Filters data within the given date range and applies MinMax scaling to 'close' -> 'scaled_close'.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== format_data for {start_date} to {end_date} ===\")\n",
    "        mask = (\n",
    "            (self.data['date'] >= pd.to_datetime(start_date)) &\n",
    "            (self.data['date'] <= pd.to_datetime(end_date))\n",
    "        )\n",
    "        data_range = self.data.loc[mask].copy()\n",
    "        print(f\"  => Data range shape: {data_range.shape}\")\n",
    "\n",
    "        if data_range.empty:\n",
    "            print(\"  WARNING: data_range is EMPTY! Check your CSV date range or date filter.\")\n",
    "            data_range['scaled_close'] = np.nan\n",
    "            return data_range\n",
    "\n",
    "        # Scale 'close'\n",
    "        data_range['scaled_close'] = self.scaler.fit_transform(data_range[['close']])\n",
    "        print(\"  => Scaled data stats:\")\n",
    "        print(\"     close.min =\", data_range['close'].min())\n",
    "        print(\"     close.max =\", data_range['close'].max())\n",
    "        print(\"     scaled_close.min =\", data_range['scaled_close'].min())\n",
    "        print(\"     scaled_close.max =\", data_range['scaled_close'].max())\n",
    "\n",
    "        # Check if scaled_close has any NaN\n",
    "        nan_count = data_range['scaled_close'].isna().sum()\n",
    "        print(f\"  => NaNs in scaled_close: {nan_count}\")\n",
    "\n",
    "        return data_range\n",
    "\n",
    "    def create_sequences(self, data, timestamps, sequence_length=144):\n",
    "        \"\"\"\n",
    "        Creates sequences of `sequence_length` (6 days). The label is the next hour after the sequence.\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        sequence_timestamps = []\n",
    "\n",
    "        for i in range(len(data) - sequence_length - 1):\n",
    "            seq = data[i : i + sequence_length, 0]\n",
    "            label = data[i + sequence_length, 0]\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "            sequence_timestamps.append(timestamps[i + sequence_length])\n",
    "\n",
    "        return np.array(sequences), np.array(labels), sequence_timestamps\n",
    "\n",
    "    def get_train_val_test(self):\n",
    "        \"\"\"\n",
    "        Splits data into training, validation, and test sets.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== get_train_val_test ===\")\n",
    "        train_data = self.format_data('2021-01-01', '2022-01-31')\n",
    "        val_data   = self.format_data('2022-02-01', '2022-02-20')\n",
    "        test_data  = self.format_data('2022-02-21', '2022-02-28')\n",
    "        \n",
    "        print(\"\\n--- Checking for empties in splits ---\")\n",
    "        print(f\"train_data.shape = {train_data.shape}\")\n",
    "        print(f\"val_data.shape   = {val_data.shape}\")\n",
    "        print(f\"test_data.shape  = {test_data.shape}\")\n",
    "\n",
    "        train_values = train_data[['scaled_close']].values\n",
    "        val_values   = val_data[['scaled_close']].values\n",
    "        test_values  = test_data[['scaled_close']].values\n",
    "\n",
    "        train_timestamps = train_data['date'].values\n",
    "        val_timestamps   = val_data['date'].values\n",
    "        test_timestamps  = test_data['date'].values\n",
    "\n",
    "        X_train, y_train, _ = self.create_sequences(train_values, train_timestamps)\n",
    "        X_val,   y_val,   _ = self.create_sequences(val_values, val_timestamps)\n",
    "        X_test,  y_test,  _ = self.create_sequences(test_values, test_timestamps)\n",
    "\n",
    "        print(\"\\n--- Shapes after create_sequences ---\")\n",
    "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"X_val:   {X_val.shape},   y_val:   {y_val.shape}\")\n",
    "        print(f\"X_test:  {X_test.shape},  y_test:  {y_test.shape}\")\n",
    "\n",
    "        # Check for NaNs:\n",
    "        print(f\"NaN count in X_train: {np.isnan(X_train).sum()}\")\n",
    "        print(f\"NaN count in y_train: {np.isnan(y_train).sum()}\")\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test,train_data, val_data, test_data\n",
    "\n",
    "    def verify_sequences(self):\n",
    "        \"\"\"\n",
    "        Uses raw 'close' data (unscaled) for a human-readable check.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== verify_sequences (raw data) ===\")\n",
    "        data_values = self.data[['close']].values\n",
    "        timestamps = self.data['date'].values\n",
    "        X, y, label_timestamps = self.create_sequences(data_values, timestamps)\n",
    "\n",
    "        print(f\"➡️ Total sequences: {len(X)}\")\n",
    "        for i in range(min(3, len(X))):\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"🔹 Sequence {i+1}\")\n",
    "            print(f\"  Start Time: {timestamps[i]}\")\n",
    "            print(f\"  End Time:   {timestamps[i + 143]}\")\n",
    "            print(f\"  Label Time: {label_timestamps[i]}\")\n",
    "            print(f\"  Sequence Values (Last 5): {X[i][-5:].flatten()} ...\")\n",
    "            print(f\"  Label Price: {y[i]}\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ddee77-ad67-4c69-8677-7e46cc0f6ec1",
   "metadata": {},
   "source": [
    "### BitcoinPriceLSTM: LSTM-Based Model for Bitcoin Price Prediction\n",
    "The BitcoinPriceLSTM class implements a Long Short-Term Memory (LSTM) neural network for predicting Bitcoin prices. It uses a Bidirectional LSTM layer to capture both past and future dependencies in sequential data, includes Batch Normalization and Dropout layers to improve generalization and prevent overfitting, and features multiple LSTM and Dense layers with ReLU activation to refine predictions. The model is compiled with the Adam optimizer and mean_squared_error loss for robust training. Key methods include build_model to construct the neural network, train to fit the model to training data, evaluate to measure performance on test data, and predict to generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "48f12f10-bb1f-44c8-8c7a-260f24306b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinPriceLSTM:\n",
    "    def __init__(self):\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Input(shape=(144, 1)))\n",
    "        model.add(Bidirectional(LSTM(units=20, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(LSTM(units=20, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(units=20, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(units=1))\n",
    "\n",
    "        model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=5, batch_size=16):\n",
    "        print(f\"\\n=== Training on {len(X_train)} samples ===\")\n",
    "        start_time = time.time()\n",
    "        self.model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            verbose=1\n",
    "        )\n",
    "        return time.time() - start_time\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e930f-e1d6-4092-9c9e-de7c491db10f",
   "metadata": {},
   "source": [
    "### ARIMA Helper Functions for Time Series Forecasting\n",
    "These helper functions facilitate the implementation of the ARIMA (AutoRegressive Integrated Moving Average) model for Bitcoin price forecasting. The prepare_arima_data(df) function converts a DataFrame containing hourly Bitcoin price data into a Pandas Series indexed by date, ensuring close price values are floats to avoid scaling issues and making the data compatible with ARIMA modeling. The walk_forward_arima(train_series, test_series, p=1, d=1, q=1) function implements a walk-forward validation approach, where the ARIMA model predicts one step ahead at each iteration while updating the training history with actual test set values. This ensures a realistic, rolling evaluation of ARIMA’s forecasting ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "82c8bbb4-d1fc-49ce-b7ce-84785fe37e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ARIMA HELPER FUNCTIONS =============\n",
    "\n",
    "def prepare_arima_data(df):\n",
    "    \"\"\"\n",
    "    Convert the hourly DataFrame (with 'date' and 'close')\n",
    "    into a Pandas Series indexed by date, suitable for ARIMA.\n",
    "    \"\"\"\n",
    "    # We assume 'df' has columns ['date', 'close'] and is sorted ascending.\n",
    "    # Set date as index\n",
    "    temp = df.copy()\n",
    "    temp.set_index('date', inplace=True)\n",
    "    # We'll ensure it's float (in case of scaling issues)\n",
    "    ts = temp['close'].astype(float)\n",
    "    return ts\n",
    "\n",
    "def walk_forward_arima(train_series, test_series, p=1, d=1, q=1):\n",
    "    \"\"\"\n",
    "    Perform a simple day-by-day (or hour-by-hour) walk-forward ARIMA forecast.\n",
    "    train_series and test_series should be Pandas Series with a datetime index.\n",
    "    \"\"\"\n",
    "    history = list(train_series.values)\n",
    "    predictions = []\n",
    "\n",
    "    # We iterate over the test set, forecast 1 step at a time\n",
    "    for t in range(len(test_series)):\n",
    "        model = ARIMA(history, order=(p, d, q))\n",
    "        model_fit = model.fit()\n",
    "        forecast_value = model_fit.forecast(steps=1)[0]\n",
    "        predictions.append(forecast_value)\n",
    "        # append the actual value from the test to the history\n",
    "        history.append(test_series.iloc[t])\n",
    "\n",
    "    # Convert predictions list -> numpy array for metrics\n",
    "    predictions = np.array(predictions)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a664b3a-46fd-462b-a8b3-cfbbb471c3ae",
   "metadata": {},
   "source": [
    "### Main Execution: LSTM vs ARIMA for Bitcoin Price Prediction\n",
    "This script executes the full pipeline, training and evaluating both an LSTM neural network and an ARIMA model to compare their performance in predicting Bitcoin prices. The workflow includes data preparation, where the dataset is loaded and preprocessed using DataLoader, and then split into training, validation, and test sets. The LSTM model is trained using different training set percentages (20%, 40%, 60%, 80%, 100%) and evaluated on the test set using Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). The ARIMA model is then trained using a walk-forward forecasting approach to simulate real-world forecasting. Finally, the performance of both models is compared using MSE and MAPE to determine which approach is more effective for Bitcoin price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "731cebc1-931a-4f9c-a354-e3a9f523bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing DataLoader ===\n",
      "Initial CSV shape: (610782, 9)\n",
      "NaNs in 'close' before resampling: 0\n",
      "Shape after resampling: (10180, 1)\n",
      "First few rows after resampling + forward-fill:\n",
      "                 date     close\n",
      "0 2021-01-01 00:00:00       NaN\n",
      "1 2021-01-01 01:00:00  29047.01\n",
      "2 2021-01-01 02:00:00  29518.58\n",
      "3 2021-01-01 03:00:00  29278.12\n",
      "4 2021-01-01 04:00:00  29386.34\n",
      "\n",
      "=== get_train_val_test ===\n",
      "\n",
      "=== format_data for 2021-01-01 to 2022-01-31 ===\n",
      "  => Data range shape: (9481, 2)\n",
      "  => Scaled data stats:\n",
      "     close.min = 28976.74\n",
      "     close.max = 68603.28\n",
      "     scaled_close.min = 0.0\n",
      "     scaled_close.max = 1.0\n",
      "  => NaNs in scaled_close: 1\n",
      "\n",
      "=== format_data for 2022-02-01 to 2022-02-20 ===\n",
      "  => Data range shape: (457, 2)\n",
      "  => Scaled data stats:\n",
      "     close.min = 36462.27\n",
      "     close.max = 45427.47\n",
      "     scaled_close.min = 0.0\n",
      "     scaled_close.max = 1.0\n",
      "  => NaNs in scaled_close: 0\n",
      "\n",
      "=== format_data for 2022-02-21 to 2022-02-28 ===\n",
      "  => Data range shape: (169, 2)\n",
      "  => Scaled data stats:\n",
      "     close.min = 34673.38\n",
      "     close.max = 39787.65\n",
      "     scaled_close.min = 0.0\n",
      "     scaled_close.max = 1.0\n",
      "  => NaNs in scaled_close: 0\n",
      "\n",
      "--- Checking for empties in splits ---\n",
      "train_data.shape = (9481, 3)\n",
      "val_data.shape   = (457, 3)\n",
      "test_data.shape  = (169, 3)\n",
      "\n",
      "--- Shapes after create_sequences ---\n",
      "X_train: (9336, 144), y_train: (9336,)\n",
      "X_val:   (312, 144),   y_val:   (312,)\n",
      "X_test:  (24, 144),  y_test:  (24,)\n",
      "NaN count in X_train: 1\n",
      "NaN count in y_train: 0\n",
      "\n",
      "--- Post get_train_val_test Stats (FULL TRAIN) ---\n",
      "X_train_full max: 1.0 | min: 0.0\n",
      "y_train_full max: 1.0 | min: 0.009262479136457502\n",
      "\n",
      "=== verify_sequences (raw data) ===\n",
      "➡️ Total sequences: 10035\n",
      "\n",
      "======================================================================\n",
      "🔹 Sequence 1\n",
      "  Start Time: 2021-01-01T00:00:00.000000000\n",
      "  End Time:   2021-01-06T23:00:00.000000000\n",
      "  Label Time: 2021-01-07T00:00:00.000000000\n",
      "  Sequence Values (Last 5): [35340.53 35094.96 36199.37 35789.44 36054.17] ...\n",
      "  Label Price: 36974.24\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "🔹 Sequence 2\n",
      "  Start Time: 2021-01-01T01:00:00.000000000\n",
      "  End Time:   2021-01-07T00:00:00.000000000\n",
      "  Label Time: 2021-01-07T01:00:00.000000000\n",
      "  Sequence Values (Last 5): [35094.96 36199.37 35789.44 36054.17 36974.24] ...\n",
      "  Label Price: 37187.82\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "🔹 Sequence 3\n",
      "  Start Time: 2021-01-01T02:00:00.000000000\n",
      "  End Time:   2021-01-07T01:00:00.000000000\n",
      "  Label Time: 2021-01-07T02:00:00.000000000\n",
      "  Sequence Values (Last 5): [36199.37 35789.44 36054.17 36974.24 37187.82] ...\n",
      "  Label Price: 36977.7\n",
      "======================================================================\n",
      "\n",
      "================== LSTM TRAINING with 20% of the Data ==================\n",
      "Subset size: 1867\n",
      "Starting training for 20% subset ...\n",
      "\n",
      "=== Training on 1867 samples ===\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 76ms/step - loss: 0.1443 - val_loss: 0.3914\n",
      "Training time: 16.00 seconds\n",
      "[20% Data] Test Loss (MSE): 0.496433\n",
      "[20% Data] MSE on Test:  0.496433\n",
      "[20% Data] MAPE on Test: 86.75%\n",
      "\n",
      "================== LSTM TRAINING with 40% of the Data ==================\n",
      "Subset size: 3734\n",
      "Starting training for 40% subset ...\n",
      "\n",
      "=== Training on 3734 samples ===\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 71ms/step - loss: 0.2335 - val_loss: 0.2741\n",
      "Training time: 24.52 seconds\n",
      "[40% Data] Test Loss (MSE): 0.360251\n",
      "[40% Data] MSE on Test:  0.360251\n",
      "[40% Data] MAPE on Test: 73.15%\n",
      "\n",
      "================== LSTM TRAINING with 60% of the Data ==================\n",
      "Subset size: 5601\n",
      "Starting training for 60% subset ...\n",
      "\n",
      "=== Training on 5601 samples ===\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 70ms/step - loss: 0.0743 - val_loss: 0.2078\n",
      "Training time: 30.93 seconds\n",
      "[60% Data] Test Loss (MSE): 0.281268\n",
      "[60% Data] MSE on Test:  0.281268\n",
      "[60% Data] MAPE on Test: 64.00%\n",
      "\n",
      "================== LSTM TRAINING with 80% of the Data ==================\n",
      "Subset size: 7468\n",
      "Starting training for 80% subset ...\n",
      "\n",
      "=== Training on 7468 samples ===\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 70ms/step - loss: 0.0544 - val_loss: 0.1637\n",
      "Training time: 40.51 seconds\n",
      "[80% Data] Test Loss (MSE): 0.227444\n",
      "[80% Data] MSE on Test:  0.227444\n",
      "[80% Data] MAPE on Test: 56.96%\n",
      "\n",
      "================== LSTM TRAINING with 100% of the Data ==================\n",
      "Subset size: 9336\n",
      "Starting training for 100% subset ...\n",
      "\n",
      "=== Training on 9336 samples ===\n",
      "\u001b[1m584/584\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 69ms/step - loss: 0.1389 - val_loss: 0.1236\n",
      "Training time: 46.77 seconds\n",
      "[100% Data] Test Loss (MSE): 0.176998\n",
      "[100% Data] MSE on Test:  0.176998\n",
      "[100% Data] MAPE on Test: 49.51%\n",
      "\n",
      "\n",
      "=== ARIMA MODEL COMPARISON ===\n",
      "Fitting ARIMA on combined (train+val) data ...\n",
      "ARIMA Test MSE:  147969.398022\n",
      "ARIMA Test MAPE: 0.70%\n",
      "\n",
      "=== Final Comparison (Test Set) ===\n",
      "LSTM Model  => MSE:  0.176998, MAPE: 49.51%\n",
      "ARIMA Model => MSE:  147969.398022, MAPE: 0.70%\n"
     ]
    }
   ],
   "source": [
    "# ============================= MAIN =============================\n",
    "if __name__ == \"__main__\":\n",
    "    data_loader = DataLoader(\"dataset.csv\")\n",
    "\n",
    "    # 1. Get the full train/val/test sets\n",
    "    X_train_full, y_train_full, X_val, y_val, X_test, y_test,df_train, df_val, df_test = data_loader.get_train_val_test()\n",
    "    \n",
    "    print(\"\\n--- Post get_train_val_test Stats (FULL TRAIN) ---\")\n",
    "    if len(X_train_full) > 0:\n",
    "        print(\"X_train_full max:\", np.nanmax(X_train_full), \"| min:\", np.nanmin(X_train_full))\n",
    "        print(\"y_train_full max:\", np.nanmax(y_train_full), \"| min:\", np.nanmin(y_train_full))\n",
    "    else:\n",
    "        print(\"X_train_full is EMPTY or invalid for the specified date range.\")\n",
    "\n",
    "    # 2. Optional: verify unscaled sequences\n",
    "    data_loader.verify_sequences()\n",
    "\n",
    "    # 3. Only proceed if we have training data\n",
    "    total_train_samples = len(X_train_full)\n",
    "    if total_train_samples == 0:\n",
    "        print(\"No training samples available. Check your CSV date range!\")\n",
    "    else:\n",
    "        # For each percentage, we re-initialize the model, train, then evaluate\n",
    "        for percentage in [20, 40, 60, 80, 100]:\n",
    "            subset_size = int(total_train_samples * (percentage / 100))\n",
    "\n",
    "            print(f\"\\n================== LSTM TRAINING with {percentage}% of the Data ==================\")\n",
    "            print(f\"Subset size: {subset_size}\")\n",
    "\n",
    "            # Subset the training data\n",
    "            X_train_sub = X_train_full[:subset_size]\n",
    "            y_train_sub = y_train_full[:subset_size]\n",
    "\n",
    "            # Build a fresh model for each percentage\n",
    "            model = BitcoinPriceLSTM()\n",
    "\n",
    "            # Train\n",
    "            print(f\"Starting training for {percentage}% subset ...\")\n",
    "            train_time = model.train(X_train_sub, y_train_sub, X_val, y_val, epochs=1, batch_size=16)\n",
    "            print(f\"Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "            # Evaluate\n",
    "            if len(X_test) > 0:\n",
    "                test_loss = model.evaluate(X_test, y_test)\n",
    "                print(f\"[{percentage}% Data] Test Loss (MSE): {test_loss:.6f}\")\n",
    "\n",
    "                # Predict\n",
    "                predictions = model.predict(X_test)\n",
    "                mse  = mean_squared_error(y_test, predictions)\n",
    "                mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "                print(f\"[{percentage}% Data] MSE on Test:  {mse:.6f}\")\n",
    "                print(f\"[{percentage}% Data] MAPE on Test: {mape:.2f}%\")\n",
    "            else:\n",
    "                print(\"No test set to evaluate on. Check date ranges!\")\n",
    "\n",
    "            if percentage ==  100:\n",
    "                best_lstm_mse = mse\n",
    "                best_lstm_mape = mape\n",
    "                best_model = model\n",
    "\n",
    "\n",
    "                # ===================== ARIMA COMPARISON =====================\n",
    "    print(\"\\n\\n=== ARIMA MODEL COMPARISON ===\")\n",
    "\n",
    "    # Prepare train/val/test data as series for ARIMA\n",
    "    # We'll combine train+val for final ARIMA training, then do walk-forward on test\n",
    "    # or you can do a walk-forward on validation as well. For simplicity, let's just do train+val -> test.\n",
    "\n",
    "    # 1) Prepare the train, val, test DataFrames for ARIMA\n",
    "    arima_train_series = prepare_arima_data(df_train)  # hourly data for train\n",
    "    arima_val_series   = prepare_arima_data(df_val)\n",
    "    arima_test_series  = prepare_arima_data(df_test)\n",
    "\n",
    "    # 2) Optionally combine train+val for final ARIMA. \n",
    "    #    If you want to do walk-forward on the entire test, you can do that directly from train.\n",
    "    combined_train_val_series = pd.concat([arima_train_series, arima_val_series])\n",
    "\n",
    "    # 3) Choose ARIMA(p, d, q). \n",
    "    #    For example, let's pick (1,1,1). You can tune this with ACF/PACF or grid search.\n",
    "    p, d, q = 1, 1, 1\n",
    "\n",
    "    print(\"Fitting ARIMA on combined (train+val) data ...\")\n",
    "    # We'll do a walk-forward approach on the test data\n",
    "    # to simulate real forecasting scenario.\n",
    "    arima_predictions = walk_forward_arima(combined_train_val_series, arima_test_series, p, d, q)\n",
    "\n",
    "    # 4) Compute ARIMA MSE and MAPE on the test set\n",
    "    # Make sure we align shapes\n",
    "    test_actual = arima_test_series.values\n",
    "    if len(test_actual) == len(arima_predictions):\n",
    "        arima_mse  = mean_squared_error(test_actual, arima_predictions)\n",
    "        arima_mape = mean_absolute_percentage_error(test_actual, arima_predictions)\n",
    "\n",
    "        print(f\"ARIMA Test MSE:  {arima_mse:.6f}\")\n",
    "        print(f\"ARIMA Test MAPE: {arima_mape:.2f}%\")\n",
    "    else:\n",
    "        print(\"Error: mismatch in length between ARIMA predictions and test data!\")\n",
    "\n",
    "    # 5) Compare ARIMA vs LSTM\n",
    "    print(\"\\n=== Final Comparison (Test Set) ===\")\n",
    "    if best_lstm_mse is not None and best_lstm_mape is not None:\n",
    "        print(f\"LSTM Model  => MSE:  {best_lstm_mse:.6f}, MAPE: {best_lstm_mape:.2f}%\")\n",
    "    else:\n",
    "        print(\"No final LSTM metrics found. Ensure you trained with 100% of the data.\")\n",
    "\n",
    "    if 'arima_mse' in locals() and 'arima_mape' in locals():\n",
    "        print(f\"ARIMA Model => MSE:  {arima_mse:.6f}, MAPE: {arima_mape:.2f}%\")\n",
    "    else:\n",
    "        print(\"No ARIMA metrics found. Check ARIMA training logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e2a91-24a5-46a2-8423-c39d466093b9",
   "metadata": {},
   "source": [
    "### Inference Time\n",
    "The inference time of the trained LSTM model was measured using 20 randomly selected test samples. Each sample was processed individually, and the time taken for prediction was recorded in milliseconds. The average inference time was computed to provide a benchmark for the model's efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f44d1456-c73f-4892-9301-71d3e333997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Measuring Inference Time with final LSTM model ===\n",
      "Average Inference Time (20 random samples): 129.41 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Measuring Inference Time with final LSTM model ===\")\n",
    "# We'll do 20 random samples from X_test\n",
    "times = []\n",
    "n_iterations = 20\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    random_index = np.random.randint(0, len(X_test))\n",
    "    random_sample = X_test[random_index]\n",
    "            \n",
    "    start_time = time.time()\n",
    "    best_model.model.predict(np.array([random_sample]), verbose=0)\n",
    "    end_time = time.time()\n",
    "            \n",
    "    # Convert to milliseconds\n",
    "    inference_time = 1000.0 * (end_time - start_time)\n",
    "    times.append(inference_time)\n",
    "\n",
    "average_inference_time = np.mean(times)\n",
    "print(f\"Average Inference Time (20 random samples): {average_inference_time:.2f} ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0e045-bca6-42c7-a892-e64f50cf05d6",
   "metadata": {},
   "source": [
    "### Final Trained Weights of the LSTM Model\r\n",
    "\r\n",
    "After training, the LSTM model's learned parameters, including weights and biases, were extracted and analyzed. Each layer of the network has a unique set of parameters whose shapes depend on the architecture of the model. Understanding these parameter dimensions helps in interpreting the model’s learned features and can be useful for debugging, fine-tuning, or applying transfer learnin.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "12ca1884-bd81-4fa0-978f-90b3b5c88e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Trained Parameters of the Neural Network ===\n",
      "\n",
      "🔹 Parameter 1: Shape (1, 80)\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan]]\n",
      "\n",
      "🔹 Parameter 2: Shape (20, 80)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "\n",
      "🔹 Parameter 3: Shape (80,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 4: Shape (1, 80)\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan]]\n",
      "\n",
      "🔹 Parameter 5: Shape (20, 80)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "\n",
      "🔹 Parameter 6: Shape (80,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 7: Shape (40,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 8: Shape (40,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 9: Shape (40,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 10: Shape (40,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 11: Shape (40, 80)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "\n",
      "🔹 Parameter 12: Shape (20, 80)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "\n",
      "🔹 Parameter 13: Shape (80,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "\n",
      "🔹 Parameter 14: Shape (20, 20)\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]]\n",
      "\n",
      "🔹 Parameter 15: Shape (20,)\n",
      "[ 0.01689691 -0.01749724 -0.01650579  0.02559517  0.00736955  0.02886982\n",
      "  0.01310627  0.02528897  0.01545702 -0.02302589 -0.02452582 -0.01766771\n",
      "  0.01602072  0.0169122   0.01569233  0.02846164 -0.02162603  0.02392101\n",
      " -0.01597609 -0.01491739]\n",
      "\n",
      "🔹 Parameter 16: Shape (20, 1)\n",
      "[[ 0.40583557]\n",
      " [-0.43678898]\n",
      " [-0.47737572]\n",
      " [ 0.09850366]\n",
      " [ 0.52415127]\n",
      " [ 0.1288176 ]\n",
      " [ 0.32186195]\n",
      " [ 0.07474694]\n",
      " [ 0.20206718]\n",
      " [-0.09903268]\n",
      " [-0.4115747 ]\n",
      " [-0.50503   ]\n",
      " [ 0.37682202]\n",
      " [ 0.2288594 ]\n",
      " [ 0.29085058]\n",
      " [ 0.02720534]\n",
      " [-0.13710192]\n",
      " [ 0.15537296]\n",
      " [-0.30073887]\n",
      " [-0.3889138 ]]\n",
      "\n",
      "🔹 Parameter 17: Shape (1,)\n",
      "[0.393732]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Trained Parameters of the Neural Network ===\")\n",
    "\n",
    "# Retrieve the weights from the LSTM model\n",
    "trained_weights = best_model.model.get_weights()\n",
    "\n",
    "for i, param in enumerate(trained_weights):\n",
    "    print(f\"\\n🔹 Parameter {i+1}: Shape {param.shape}\")\n",
    "    print(param)  # Prints the actual numerical values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a135f1-6a04-49d2-a114-19446a439a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
